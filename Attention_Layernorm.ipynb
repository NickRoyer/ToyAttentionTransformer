{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this workbook shows the impact of layernorm and mlp on attention tensors with simplified examples\n",
    "\n",
    "import torch\n",
    "from model_details import LayerNorm, CausalSelfAttention, MLP, Block\n",
    "\n",
    "class ToyConfig:\n",
    "    block_size: int = 4\n",
    "    vocab_size: int = 12 # this is a toy the vocab is limited to a handful of letters that make a bunch of 4 letter words\n",
    "    n_layer: int = 1\n",
    "    n_head: int = 1\n",
    "    n_embd: int = 6\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = False # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n",
    "    printModel: bool = True # True prints the model to debug\n",
    "\n",
    "config = ToyConfig()\n",
    "blk = Block(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.7238, -1.3490,  2.2708,  0.1644,  0.4643, -0.9596],\n",
      "         [ 0.3577,  0.2595, -1.0803,  0.4079,  0.1390, -1.1237],\n",
      "         [-1.9288, -0.8133,  0.6529,  0.1452,  0.1837, -2.2137],\n",
      "         [ 0.0828, -1.8399, -0.7917, -0.6395,  0.5560, -0.2244]]])\n"
     ]
    }
   ],
   "source": [
    "# Define the given tensor\n",
    "x = torch.randn(1, 4, 6)\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y : \n",
      " tensor([[[ 0.8407, -1.2674,  2.1852,  0.0537,  0.2427, -0.7833],\n",
      "         [ 0.3849,  0.3378, -1.3973,  0.7551,  0.0442, -0.9987],\n",
      "         [-1.5932, -0.7877,  0.3467,  0.4331, -0.2252, -1.9986],\n",
      "         [ 0.1146, -1.6984, -0.8226, -0.3603,  0.4504,  0.0197]]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = blk(x) #Note since we have not done any training the output on this is relatively random\n",
    "print (\"Y : \\n\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "attn = CausalSelfAttention(config)\n",
    "ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "mlp = MLP(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before first layernorm: \n",
      " tensor([[[ 0.7238, -1.3490,  2.2708,  0.1644,  0.4643, -0.9596],\n",
      "         [ 0.3577,  0.2595, -1.0803,  0.4079,  0.1390, -1.1237],\n",
      "         [-1.9288, -0.8133,  0.6529,  0.1452,  0.1837, -2.2137],\n",
      "         [ 0.0828, -1.8399, -0.7917, -0.6395,  0.5560, -0.2244]]])\n",
      "after first layernorm: \n",
      " tensor([[[ 0.4272, -1.3272,  1.7365, -0.0463,  0.2075, -0.9977],\n",
      "         [ 0.8019,  0.6537, -1.3697,  0.8778,  0.4716, -1.4354],\n",
      "         [-1.1617, -0.1385,  1.2064,  0.7407,  0.7760, -1.4230],\n",
      "         [ 0.7395, -1.8045, -0.4176, -0.2161,  1.3656,  0.3330]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"before first layernorm: \\n\", x)\n",
    "z = ln_1(x)\n",
    "print(\"after first layernorm: \\n\", z)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: \n",
      " tensor([[[ 0.7238, -1.3490,  2.2708,  0.1644,  0.4643, -0.9596],\n",
      "         [ 0.3577,  0.2595, -1.0803,  0.4079,  0.1390, -1.1237],\n",
      "         [-1.9288, -0.8133,  0.6529,  0.1452,  0.1837, -2.2137],\n",
      "         [ 0.0828, -1.8399, -0.7917, -0.6395,  0.5560, -0.2244]]])\n",
      "Just Attention: \n",
      " tensor([[[ 0.1134, -0.0554, -0.0975, -0.2883,  0.3562,  0.4833],\n",
      "         [ 0.1387, -0.0517, -0.0601, -0.1950,  0.3259,  0.3051],\n",
      "         [ 0.1496, -0.0027, -0.0425, -0.3056,  0.3246,  0.3577],\n",
      "         [ 0.1379,  0.0094, -0.0092, -0.3401,  0.2982,  0.3635]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "With layernorm: \n",
      " tensor([[[ 0.0598, -0.0578, -0.0833, -0.2837,  0.3234,  0.3881],\n",
      "         [ 0.1718, -0.0576, -0.0611, -0.1994,  0.3682,  0.3163],\n",
      "         [ 0.2174,  0.0009, -0.0398, -0.2464,  0.3312,  0.3657],\n",
      "         [ 0.2003,  0.0136, -0.0110, -0.2572,  0.2743,  0.3961]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "just_attn = attn(x)\n",
    "ln_attn = attn(z)\n",
    "\n",
    "print(\"X: \\n\", x)\n",
    "print('Just Attention: \\n', just_attn)\n",
    "print('With layernorm: \\n', ln_attn)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
