{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Step 1: Define Logits and Targets"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Description:** Logits are raw scores (before softmax) produced by the model for each class (or for LLMs token score). Targets are the ground-truth class labels (as indices of the token in the lookup table)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Logits': array([[2. , 1. , 0.1],\n",
      "       [0.5, 2.5, 0.3]]), 'Targets': array([0, 1])}\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Step 2: Apply Softmax to Convert Logits into Probabilities"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Description:** Softmax normalizes logits into probabilities for each class, ensuring they sum to 1."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.65900114 0.24243297 0.09856589]\n",
      " [0.10860373 0.80247906 0.08891721]]\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Step 3: Extract Correct Class Probabilities"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Description:** We select the probability assigned to the true class for each sample."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.65900114 0.80247906]\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Step 4: Compute Negative Log-Likelihoods"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Description:** The loss function penalizes incorrect predictions by taking the negative log of the correct class probability. Lower probabilities result in higher penalties. What's critical to note is this effect is spread amongst incorrect probabilities"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.41703002 0.22004952]\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Step 5: Compute Final Cross-Entropy Loss"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Description:** The final loss is computed as the mean of all negative log-likelihood values across all samples. A lower value indicates better model performance."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3185397696491857\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Step 6: Compute Cross-Entropy Loss using PyTorch"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Description:** We verify our manual calculation by comparing it to PyTorch’s built-in cross_entropy function."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31853973865509033\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Helper function to format and display each step clearly\n",
    "def print_step(title, description, value):\n",
    "    display(Markdown(f\"## {title}\"))\n",
    "    display(Markdown(f\"**Description:** {description}\"))\n",
    "    print(value)\n",
    "    print(\"\\n\" + \"-\"*60 + \"\\n\")\n",
    "\n",
    "# Step 2: Define sample logits and target labels\n",
    "logits = np.array([[2.0, 1.0, 0.1],  # Class scores for sample 1\n",
    "                   [0.5, 2.5, 0.3]])  # Class scores for sample 2\n",
    "\n",
    "targets = np.array([0, 1])  # True class labels (sample 1 -> class 0, sample 2 -> class 1)\n",
    "\n",
    "print_step(\n",
    "    \"Step 1: Define Logits and Targets\",\n",
    "    \"Logits are raw scores (before softmax) produced by the model for each class (or for LLMs token score).\"\n",
    "    \" Targets are the ground-truth class labels (as indices of the token in the lookup table).\",\n",
    "    {\"Logits\": logits, \"Targets\": targets}\n",
    ")\n",
    "\n",
    "# Step 3: Convert logits to probabilities using Softmax\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))  # Subtract max for numerical stability\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "probabilities = softmax(logits)\n",
    "\n",
    "print_step(\n",
    "    \"Step 2: Apply Softmax to Convert Logits into Probabilities\",\n",
    "    \"Softmax normalizes logits into probabilities for each class, ensuring they sum to 1.\",\n",
    "    probabilities\n",
    ")\n",
    "\n",
    "# Step 4: Extract the probabilities of the correct classes\n",
    "correct_class_probs = probabilities[np.arange(len(targets)), targets]\n",
    "\n",
    "print_step(\n",
    "    \"Step 3: Extract Correct Class Probabilities\",\n",
    "    \"We select the probability assigned to the true class for each sample.\",\n",
    "    correct_class_probs\n",
    ")\n",
    "\n",
    "# Step 5: Compute the Negative Log Likelihood (NLL)\n",
    "negative_log_likelihoods = -np.log(correct_class_probs)\n",
    "\n",
    "print_step(\n",
    "    \"Step 4: Compute Negative Log-Likelihoods\",\n",
    "    \"The loss function penalizes incorrect predictions by taking the negative log \"\n",
    "    \"of the correct class probability. Lower probabilities result in higher penalties. What's critical to note is this effect is spread amongst incorrect probabilities\",\n",
    "    negative_log_likelihoods\n",
    ")\n",
    "\n",
    "# Step 6: Compute Final Cross-Entropy Loss\n",
    "cross_entropy_loss = np.mean(negative_log_likelihoods)\n",
    "\n",
    "print_step(\n",
    "    \"Step 5: Compute Final Cross-Entropy Loss\",\n",
    "    \"The final loss is computed as the mean of all negative log-likelihood values \"\n",
    "    \"across all samples. A lower value indicates better model performance.\",\n",
    "    cross_entropy_loss\n",
    ")\n",
    "\n",
    "# ---------------------------------------\n",
    "# Step 7: Compute Cross-Entropy using PyTorch for Comparison\n",
    "logits_torch = torch.tensor(logits, dtype=torch.float32)\n",
    "targets_torch = torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "torch_ce_loss = F.cross_entropy(logits_torch, targets_torch).item()\n",
    "\n",
    "print_step(\n",
    "    \"Step 6: Compute Cross-Entropy Loss using PyTorch\",\n",
    "    \"We verify our manual calculation by comparing it to PyTorch’s built-in cross_entropy function.\",\n",
    "    torch_ce_loss\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
