{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found vocab_size = 11 (inside data\\toy\\meta.pkl)\n",
      "number of parameters: 0.00M\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#This sample is adapted from the work provided here: https://github.com/karpathy/nanoGPT\n",
    "#removed some of the more complicated concepts (learning rate decay, gradiant accumulation steps, dps support, configuration management etc for a clearer sample)\n",
    "#refactored to create a training class responsible for training the model, and a dataloader to fetch data\n",
    "\n",
    "# There are 2 datasets to train from, the character representation of shakespeare which yields a vocab size of 65\n",
    "# The toy dataset which has a vocabulary of 11\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from config import ToyConfig, ToyTrainingConfig\n",
    "from config import ToyTrainingConfig\n",
    "from toy_model import ToyModel\n",
    "from data_loader import DataLoader\n",
    "from toy_trainer import ToyTrainer\n",
    "\n",
    "modelConfig = ToyConfig()\n",
    "#override any default values here: \n",
    "\n",
    "trainingConfig = ToyTrainingConfig()\n",
    "trainingConfig.block_size = modelConfig.block_size # Need to set to ToyConfig.block_size\n",
    "\n",
    "# name of the dataset to use\n",
    "dataset = 'toy'\n",
    "\n",
    "data = DataLoader(dataset=dataset)\n",
    "modelConfig.vocab_size = data.get_vocab_size()\n",
    "trainingConfig.vocab_size = modelConfig.vocab_size\n",
    "model = ToyModel(modelConfig)\n",
    "trainer = ToyTrainer(trainingConfig, model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: loss 2.3001, time 57.61ms\n",
      "iter 100: loss 1.2526, time 7.81ms\n",
      "iter 200: loss 0.5879, time 4.88ms\n",
      "iter 300: loss 0.3571, time 8.78ms\n",
      "iter 400: loss 0.2281, time 6.84ms\n",
      "iter 500: loss 0.1400, time 5.87ms\n",
      "iter 600: loss 0.1189, time 8.79ms\n",
      "iter 700: loss 0.1120, time 4.88ms\n",
      "iter 800: loss 0.0956, time 7.81ms\n",
      "iter 900: loss 0.0670, time 5.86ms\n",
      "iter 1000: loss 0.0702, time 6.84ms\n"
     ]
    }
   ],
   "source": [
    "#train the model some so that the results aren't random\n",
    "\n",
    "##### NOTE: Training for more iterations will increase the probability of a correct answer try with larger trainning iterations to see the improvement:\n",
    "# also important to note this is such a small data set \n",
    "\n",
    "#batch_size, number of iterations, log every x iterations\n",
    "trainer.TrainingLoop(20, 1000, 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N  : Target single Character according to source input \n",
      "\n",
      "Input: \n",
      " COR \n",
      "\n",
      "Target: \n",
      " CORN\n",
      "\n",
      "X: \n",
      " tensor([[2, 7, 8, 0]])\n",
      "\n",
      "Y: \n",
      " tensor([[2, 7, 8, 6]])\n"
     ]
    }
   ],
   "source": [
    "# you can repeat this as nessary until you get a good input\n",
    "X, Y = trainer.get_single_batch() #get_single_batch gets validation data so this is data the model has not seen during training\n",
    "\n",
    "B, T = Y.size()\n",
    "target = Y[0][T-1].tolist()\n",
    "print(data.decode([target]), ' : Target single Character according to source input \\n')\n",
    "\n",
    "# The target is one character less then the target, the number of characters shown is based on block size\n",
    "print(\"Input: \\n\",data.decode(X.flatten().tolist()))\n",
    "print(\"\\nTarget: \\n\",data.decode(Y.flatten().tolist()))\n",
    "\n",
    "print(\"\\nX: \\n\", X)\n",
    "print(\"\\nY: \\n\", Y)\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of the Model vs the Target input: **Note: rerunning this will do a backwards pass and thus improve the output \n",
      "\n",
      "C -> [\"' ':0.1%\", \"'A':0.1%\", \"'C':98.6%\", \"'E':0.1%\", \"'I':0.2%\", \"'L':0.1%\", \"'N':0.3%\", \"'O':0.1%\", \"'R':0.1%\", \"'S':0.0%\", \"'T':0.2%\"]\n",
      "O -> [\"' ':0.1%\", \"'A':0.1%\", \"'C':0.1%\", \"'E':0.1%\", \"'I':0.1%\", \"'L':0.1%\", \"'N':0.1%\", \"'O':98.8%\", \"'R':0.1%\", \"'S':0.1%\", \"'T':0.3%\"]\n",
      "R -> [\"' ':0.1%\", \"'A':0.1%\", \"'C':0.1%\", \"'E':0.1%\", \"'I':0.1%\", \"'L':0.0%\", \"'N':0.0%\", \"'O':0.1%\", \"'R':99.0%\", \"'S':0.1%\", \"'T':0.2%\"]\n",
      "N -> [\"' ':0.1%\", \"'A':1.3%\", \"'C':0.6%\", \"'E':3.1%\", \"'I':0.2%\", \"'L':0.2%\", \"'N':91.5%\", \"'O':0.1%\", \"'R':0.2%\", \"'S':1.2%\", \"'T':1.5%\"]\n",
      "\n",
      "Data Target (decoded): \n",
      " CORN\n",
      "\n",
      "Model Result (decoded): \n",
      " CORN\n",
      "\n",
      "Last Char of the Target: N\n",
      "Last Char of the  Model: N\n",
      "\n",
      "This pass: Succeeded\n",
      "\n",
      "Loss:  0.030869895592331886\n"
     ]
    }
   ],
   "source": [
    "# everytime this step is run the model will learn the input\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def format_probs_tensor_as_percent(probs_tensor):\n",
    "    \"\"\"\n",
    "    Converts a PyTorch tensor of probabilities into a percentage format for display.\n",
    "    If probs_tensor is 1D (shape [vocab_size]), we treat it as a single row.\n",
    "    If it's 2D (shape [N, vocab_size]), we handle multiple rows.\n",
    "    \n",
    "    :param probs_tensor: PyTorch tensor containing probability values (0 to 1).\n",
    "    :return: A list of lists of formatted percentage strings.\n",
    "    \"\"\"\n",
    "    # 1) Convert to percentage scale and cast to float\n",
    "    probs_percent = (probs_tensor * 100).to(torch.float32)\n",
    "    \n",
    "    # 2) Ensure 2D for uniform processing\n",
    "    #    If shape is (vocab_size,), make it (1, vocab_size)\n",
    "    if probs_percent.dim() == 1:\n",
    "        probs_percent = probs_percent.unsqueeze(0)\n",
    "    \n",
    "    # Build the vocabulary array (assuming modelConfig.vocab_size and data are in scope)\n",
    "    vocab_arr = [data.decode([i]) for i in range(modelConfig.vocab_size)]\n",
    "\n",
    "    # 3) Convert each probability to a formatted string with two decimal places\n",
    "    #    We'll return a 2D list, one sub-list per row\n",
    "    formatted_probs = [\n",
    "        [\n",
    "            f\"'{vocab_arr[idx]}':{float(prob):.1f}%\"\n",
    "            for idx, prob in enumerate(row)\n",
    "        ]\n",
    "        for row in probs_percent\n",
    "    ]\n",
    "    \n",
    "    return formatted_probs\n",
    "\n",
    "def format_model_results(target, logits, printCount):\n",
    "    # Convert logits -> probabilities\n",
    "    allProbs = F.softmax(logits, dim=-1)  # shape: (B, T, vocab_size)\n",
    "    allPicks = torch.argmax(allProbs, dim=-1)  # shape: (B, T)\n",
    "\n",
    "    # batch size = 1 for simplicity\n",
    "    B, T, V = allProbs.shape\n",
    "    assert B == 1, \"This snippet assumes batch size = 1\"\n",
    "\n",
    "    # Determine the range for the last 'printCount' positions\n",
    "    #    e.g. if T=10 and printCount=4, we want positions [6..9].\n",
    "    start_idx = max(0, T - printCount)\n",
    "    \n",
    "    for i in range(start_idx, T):\n",
    "        # The chosen token index at position i\n",
    "        pick_idx = allPicks[0, i].item()\n",
    "        # Decode that index into a character\n",
    "        char = data.decode([pick_idx])\n",
    "        \n",
    "        # 3) Format probabilities for that position\n",
    "        #    shape (vocab_size,) => pass to our function\n",
    "        probs_i = allProbs[0, i]\n",
    "        \n",
    "        # format_probs_tensor_as_percent returns a list of lists;\n",
    "        # with a single row, we can index [0] to get the list of per-token strings\n",
    "        formatted = format_probs_tensor_as_percent(probs_i)[0]\n",
    "        \n",
    "        print(f\"{char} -> {formatted}\")\n",
    "\n",
    "    # 3) Decode the entire sequences\n",
    "    decoded_target = data.decode(target.flatten().tolist())\n",
    "    decoded_model  = data.decode(allPicks.flatten().tolist())\n",
    "\n",
    "    print(\"\\nData Target (decoded): \\n\", decoded_target)\n",
    "    print(\"\\nModel Result (decoded): \\n\", decoded_model)\n",
    "\n",
    "    # 4) Print the last character of each\n",
    "    print(f\"\\nLast Char of the Target: {decoded_target[-1]}\")    \n",
    "    print(f\"Last Char of the  Model: {decoded_model[-1]}\")\n",
    "\n",
    "    success = decoded_target[-1] == decoded_model[-1]\n",
    "    resutlStr = 'Succeeded' if success else 'Failed'\n",
    "    print('\\nThis pass: '+ resutlStr)\n",
    "\n",
    "# -- After running the model --\n",
    "logits, loss = model(X, Y) \n",
    "\n",
    "print('Results of the Model vs the Target input: **Note: rerunning this will do a backwards pass and thus improve the output \\n')\n",
    "\n",
    "format_model_results(Y, logits, 5)\n",
    "\n",
    "print('\\nLoss: ', loss.item())\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "trainer.optimizer.step()\n",
    "\n",
    "# flush the gradients as soon as we can, no need for this memory anymore\n",
    "trainer.optimizer.zero_grad(set_to_none=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of model logits:  torch.Size([1, 4, 11])\n"
     ]
    }
   ],
   "source": [
    "# see the raw model logits, this shows the value of the logits at all positions and not just the final position that we are uttilizing for projection of the final answer\n",
    "\n",
    "print(\"Size of model logits: \", logits.size()) # Batch, Block_Size, Vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Educated Guess:  3\n",
      "Highest Probability:  2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "#This block shows a sample of the difference between using an \"Educated Guess\" vs a \"Highest Probability\"\n",
    "\n",
    "# Define probability distribution\n",
    "probs = torch.tensor([0.1, 0.3, 0.4, 0.2])  # Must be non-negative\n",
    "\n",
    "# Educated Guess: Sample 1 events without replacement \n",
    "educated_guess = torch.multinomial(probs, num_samples=1, replacement=False).item()  # using .item() becasue we are only doing a single sample for comparis\n",
    "\n",
    "highest_probablity = torch.argmax(probs).item() \n",
    "\n",
    "print('Educated Guess: ', educated_guess) \n",
    "print('Highest Probability: ', highest_probablity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
