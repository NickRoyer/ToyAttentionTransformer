{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found vocab_size = 11 (inside data\\toy\\meta.pkl)\n",
      "number of parameters: 0.00M\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#This sample is adapted from the work provided here: https://github.com/karpathy/nanoGPT\n",
    "#removed some of the more complicated concepts (learning rate decay, gradiant accumulation steps, dps support, configuration management etc for a clearer sample)\n",
    "#refactored to create a training class responsible for training the model, and a dataloader to fetch data\n",
    "\n",
    "# There are 2 datasets to train from, the character representation of shakespeare which yields a vocab size of 65\n",
    "# The toy dataset which has a vocabulary of 11\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from config import ToyConfig, ToyTrainingConfig\n",
    "from config import ToyTrainingConfig\n",
    "from toy_model import ToyModel\n",
    "from data_loader import DataLoader\n",
    "from toy_trainer import ToyTrainer\n",
    "\n",
    "modelConfig = ToyConfig()\n",
    "#override any default values here: \n",
    "\n",
    "trainingConfig = ToyTrainingConfig()\n",
    "trainingConfig.block_size = modelConfig.block_size # Need to set to ToyConfig.block_size\n",
    "\n",
    "# name of the dataset to use\n",
    "dataset = 'toy'\n",
    "\n",
    "data = DataLoader(dataset=dataset)\n",
    "modelConfig.vocab_size = data.get_vocab_size()\n",
    "trainingConfig.vocab_size = modelConfig.vocab_size\n",
    "model = ToyModel(modelConfig)\n",
    "trainer = ToyTrainer(trainingConfig, model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: loss 0.2082, time 8.79ms\n",
      "iter 10: loss 0.1496, time 7.81ms\n",
      "iter 20: loss 0.2383, time 6.83ms\n",
      "iter 30: loss 0.2267, time 7.81ms\n",
      "iter 40: loss 0.4842, time 9.76ms\n",
      "iter 50: loss 0.1686, time 7.81ms\n",
      "iter 60: loss 0.2581, time 7.81ms\n",
      "iter 70: loss 0.2301, time 6.84ms\n",
      "iter 80: loss 0.2345, time 7.81ms\n",
      "iter 90: loss 0.1339, time 7.81ms\n",
      "iter 100: loss 0.1890, time 7.81ms\n"
     ]
    }
   ],
   "source": [
    "#train the model some so that the results aren't random\n",
    "\n",
    "##### NOTE: Training for more iterations will increase the probability of a correct answer try with larger trainning iterations to see the improvement:\n",
    "# also important to note this is such a small data set \n",
    "\n",
    "#batch_size, number of iterations, log every x iterations\n",
    "trainer.TrainingLoop(4, 100, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R  : Target single Character according to source input \n",
      "\n",
      "Input: \n",
      " NEA \n",
      "\n",
      "Target: \n",
      " NEAR\n",
      "\n",
      "X: \n",
      " tensor([[6, 3, 1, 0]])\n",
      "\n",
      "Y: \n",
      " tensor([[6, 3, 1, 8]])\n"
     ]
    }
   ],
   "source": [
    "# you can repeat this as nessary until you get a good input\n",
    "X, Y = trainer.get_single_batch() #get_single_batch gets validation data so this is data the model has not seen during training\n",
    "\n",
    "B, T = Y.size()\n",
    "target = Y[0][T-1].tolist()\n",
    "print(data.decode([target]), ' : Target single Character according to source input \\n')\n",
    "\n",
    "# The target is one character less then the target, the number of characters shown is based on block size\n",
    "print(\"Input: \\n\",data.decode(X.flatten().tolist()))\n",
    "print(\"\\nTarget: \\n\",data.decode(Y.flatten().tolist()))\n",
    "\n",
    "print(\"\\nX: \\n\", X)\n",
    "print(\"\\nY: \\n\", Y)\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R  : Actual Target ( 8 )\n",
      "R  : Educated guess by picking amongst the highest probabilities ( 8 )\n",
      "   : Pick based solely on highest probability ( 0 )\n"
     ]
    }
   ],
   "source": [
    "mod_logits, loss = model(X, Y) \n",
    "logits = mod_logits[:, -1, :] # pluck the logits at the final step \n",
    "\n",
    "# apply softmax to convert logits to (normalized) probabilities\n",
    "probs = F.softmax(logits, dim=-1)\n",
    "# sample from the distribution\n",
    "idx_guess = torch.multinomial(probs, num_samples=1)\n",
    "idx_guess= idx_guess.flatten().tolist() # This formats the ouptut into what is needed for decoding\n",
    "\n",
    "# Argmax: always picks the highest probability word\n",
    "idx_pick = torch.argmax(probs).item() \n",
    "idx_pick = list([idx_pick]) # same this formats for decoding\n",
    "\n",
    "guess = data.decode(idx_guess)\n",
    "pick = data.decode(idx_pick)\n",
    "print(data.decode([target]), ' : Actual Target (', target, ')')\n",
    "print(guess, \" : Educated guess by picking amongst the highest probabilities (\", idx_guess[0], ')')\n",
    "print(pick, \" : Pick based solely on highest probability (\", idx_pick[0], ')')\n",
    "\n",
    "# Re running this block will re run the input through the model (note that this is just inference and not training so it does not learn the answer)\n",
    "# after multiple runs the probability pick should not change however the educated guess might guess the proper value\n",
    "# in addition to increase the likely hood of success run the training loop for more iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits: \n",
      " tensor([[ 0.8495, -0.0207, -0.7849,  1.1798, -0.3315, -0.4967, -0.2611, -0.9563,\n",
      "         -0.1560,  0.1018, -0.1526]], grad_fn=<SliceBackward0>)\n",
      "Probs: \n",
      " tensor([[0.1897, 0.0794, 0.0370, 0.2639, 0.0582, 0.0494, 0.0625, 0.0312, 0.0694,\n",
      "         0.0898, 0.0696]], grad_fn=<SoftmaxBackward0>)\n",
      "Probs formatted: \n",
      " [['18.97%', '7.94%', '3.70%', '26.39%', '5.82%', '4.94%', '6.25%', '3.12%', '6.94%', '8.98%', '6.96%']]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "def format_probs_tensor_as_percent(probs_tensor):\n",
    "    \"\"\"\n",
    "    Converts a PyTorch tensor of probabilities into a percentage format for display.\n",
    "    Handles 2D tensors by processing each row separately.\n",
    "\n",
    "    :param probs_tensor: PyTorch tensor containing probability values (0 to 1).\n",
    "    :return: List of formatted percentages as strings.\n",
    "    \"\"\"\n",
    "    probs_percent = (probs_tensor * 100).to(torch.float32)  # Convert to percentage scale\n",
    "    \n",
    "    # Convert each probability to a formatted string with two decimal places\n",
    "    formatted_probs = [[f\"{float(p):.2f}%\" for p in row] for row in probs_percent]\n",
    "\n",
    "    return formatted_probs\n",
    "\n",
    "print(\"logits: \\n\", logits)\n",
    "print('Probs: \\n', probs)\n",
    "\n",
    "print('Probs formatted: \\n', format_probs_tensor_as_percent(probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of model logits:  torch.Size([1, 4, 11])\n",
      "model logits: \n",
      " tensor([[[-0.1753,  0.2426,  0.1460, -0.2412,  0.0476, -0.2525,  1.1364,\n",
      "          -0.1690, -0.4356, -0.1286,  0.1148],\n",
      "         [-0.2818, -0.3707,  0.2225,  0.1348,  1.0175, -0.1664, -0.0777,\n",
      "           0.1758, -0.4975,  0.1709, -0.4587],\n",
      "         [-0.4698, -0.0209,  1.2362, -0.4932,  0.3155,  0.1761,  0.1195,\n",
      "           0.1041, -0.0980,  0.0945,  0.0176],\n",
      "         [ 0.7953, -0.2032, -0.7338,  1.2673, -0.1755, -0.5247, -0.3400,\n",
      "          -0.8945, -0.2268,  0.1289, -0.2089]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# see the raw model logits, this shows the value of the logits at all positions and not just the final position that we are uttilizing for projection of the final answer\n",
    "\n",
    "print(\"Size of model logits: \", mod_logits.size()) # Batch, Block_Size, Vocabulary \n",
    "print(\"model logits: \\n\", mod_logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Educated Guess:  2\n",
      "Highest Probability:  2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "#This block shows a sample of the difference between using an \"Educated Guess\" vs a \"Highest Probability\"\n",
    "\n",
    "# Define probability distribution\n",
    "probs = torch.tensor([0.1, 0.3, 0.4, 0.2])  # Must be non-negative\n",
    "\n",
    "# Educated Guess: Sample 1 events without replacement \n",
    "educated_guess = torch.multinomial(probs, num_samples=1, replacement=False).item()  # using .item() becasue we are only doing a single sample for comparis\n",
    "\n",
    "highest_probablity = torch.argmax(probs).item() \n",
    "\n",
    "print('Educated Guess: ', educated_guess) \n",
    "print('Highest Probability: ', highest_probablity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
