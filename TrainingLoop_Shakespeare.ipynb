{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found vocab_size = 65 (inside data\\shakespeare\\meta.pkl)\n",
      "number of parameters: 0.21M\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#this sample is adapted from the work provided here: https://github.com/karpathy/nanoGPT\n",
    "#removed some of the more complicated concepts (learning rate decay, gradiant accumulation steps, dps support, configuration management etc for a clearer sample)\n",
    "#refactored to create a training class responsible for training the model, and a dataloader to fetch data\n",
    "\n",
    "# There are 2 datasets to train from, the character representation of shakespeare which yields a vocab size of 65\n",
    "# The toy dataset which has a vocabulary of 11\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from config import ToyConfig, ToyTrainingConfig\n",
    "from config import ToyTrainingConfig\n",
    "from toy_model import ToyModel\n",
    "from data_loader import DataLoader\n",
    "from toy_trainer import ToyTrainer\n",
    "\n",
    "modelConfig = ToyConfig()\n",
    "#override any default values here: \n",
    "# example overrides for CPU processing of the shakespeare dataset.\n",
    "modelConfig.block_size= 64\n",
    "modelConfig.n_layer = 4 # Number of layers within the transformer\n",
    "modelConfig.n_head = 2 # Number Attention heads\n",
    "modelConfig.n_embd = 128 # Number of C elements in the B, T, C Tensor that is used in the model ( B = Batch Size, T = Time position or Sequence Length, C = Number of embed elements )\n",
    "\n",
    "\n",
    "trainingConfig = ToyTrainingConfig()\n",
    "trainingConfig.block_size = modelConfig.block_size # Need to set to ToyConfig.block_size\n",
    "\n",
    "# name of the dataset to use ('toy' or 'shakespeare')\n",
    "dataset = 'shakespeare'\n",
    "\n",
    "data = DataLoader(dataset=dataset)\n",
    "modelConfig.vocab_size = data.get_vocab_size()\n",
    "trainingConfig.vocab_size = modelConfig.vocab_size\n",
    "model = ToyModel(modelConfig)\n",
    "trainer = ToyTrainer(trainingConfig, model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: loss 1.7167, time 93.75ms\n",
      "iter 200: loss 1.5815, time 77.14ms\n",
      "iter 400: loss 1.6041, time 93.75ms\n",
      "iter 600: loss 1.6452, time 92.77ms\n",
      "iter 800: loss 1.6051, time 65.44ms\n",
      "iter 1000: loss 1.5516, time 84.47ms\n",
      "iter 1200: loss 1.5898, time 77.15ms\n",
      "iter 1400: loss 1.5768, time 89.85ms\n",
      "iter 1600: loss 1.6485, time 86.92ms\n",
      "iter 1800: loss 1.6271, time 61.53ms\n",
      "iter 2000: loss 1.6035, time 78.13ms\n"
     ]
    }
   ],
   "source": [
    "#train the model some so that the results aren't random\n",
    "\n",
    "##### NOTE: Training for more iterations will increase the probability of a correct answer try with larger trainning iterations to see the improvement:\n",
    "# also important to note this is such a small data set \n",
    "\n",
    "#shakespeare\n",
    "#batch_size, number of iterations, log every x iterations\n",
    "trainer.TrainingLoop(24, 2000, 200) #takes approx 4 minutes  on an older PC\n",
    "\n",
    "#running this loop 1 time yields a loss of approx 1.7 +/- .15\n",
    "#running it 2 times yields a training loss of approx 1.6 +/- .30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e  : Target single Character according to source input \n",
      "\n",
      "Input: \n",
      " is true?\n",
      "\n",
      "ARIEL:\n",
      "Ay, sir.\n",
      "\n",
      "PROSPERO:\n",
      "This blue-eyed hag was hith\n",
      "\n",
      "Target: \n",
      " s true?\n",
      "\n",
      "ARIEL:\n",
      "Ay, sir.\n",
      "\n",
      "PROSPERO:\n",
      "This blue-eyed hag was hithe\n",
      "\n",
      "X: \n",
      " tensor([[47, 57,  1, 58, 56, 59, 43, 12,  0,  0, 13, 30, 21, 17, 24, 10,  0, 13,\n",
      "         63,  6,  1, 57, 47, 56,  8,  0,  0, 28, 30, 27, 31, 28, 17, 30, 27, 10,\n",
      "          0, 32, 46, 47, 57,  1, 40, 50, 59, 43,  7, 43, 63, 43, 42,  1, 46, 39,\n",
      "         45,  1, 61, 39, 57,  1, 46, 47, 58, 46]])\n",
      "\n",
      "Y: \n",
      " tensor([[57,  1, 58, 56, 59, 43, 12,  0,  0, 13, 30, 21, 17, 24, 10,  0, 13, 63,\n",
      "          6,  1, 57, 47, 56,  8,  0,  0, 28, 30, 27, 31, 28, 17, 30, 27, 10,  0,\n",
      "         32, 46, 47, 57,  1, 40, 50, 59, 43,  7, 43, 63, 43, 42,  1, 46, 39, 45,\n",
      "          1, 61, 39, 57,  1, 46, 47, 58, 46, 43]])\n"
     ]
    }
   ],
   "source": [
    "# you can repeat this as nessary until you get a good input\n",
    "X, Y = trainer.get_single_batch() #get_single_batch gets validation data so this is data the model has not seen during training\n",
    "\n",
    "B, T = Y.size()\n",
    "target = Y[0][T-1].tolist()\n",
    "print(data.decode([target]), ' : Target single Character according to source input \\n')\n",
    "\n",
    "# The target is one character less then the target, the number of characters shown is based on block size\n",
    "print(\"Input: \\n\",data.decode(X.flatten().tolist()))\n",
    "print(\"\\nTarget: \\n\",data.decode(Y.flatten().tolist()))\n",
    "\n",
    "print(\"\\nX: \\n\", X)\n",
    "print(\"\\nY: \\n\", Y)\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e  : Actual Target ( 43 )\n",
      "   : Educated guess by picking amongst the highest probabilities ( 1 )\n",
      "e  : Pick based solely on highest probability ( 43 )\n"
     ]
    }
   ],
   "source": [
    "mod_logits, loss = model(X, Y) \n",
    "logits = mod_logits[:, -1, :] # pluck the logits at the final step \n",
    "\n",
    "# apply softmax to convert logits to (normalized) probabilities\n",
    "probs = F.softmax(logits, dim=-1)\n",
    "# sample from the distribution\n",
    "idx_guess = torch.multinomial(probs, num_samples=1)\n",
    "idx_guess= idx_guess.flatten().tolist() # This formats the ouptut into what is needed for decoding\n",
    "\n",
    "# Argmax: always picks the highest probability word\n",
    "idx_pick = torch.argmax(probs).item() \n",
    "idx_pick = list([idx_pick]) # same this formats for decoding\n",
    "\n",
    "guess = data.decode(idx_guess)\n",
    "pick = data.decode(idx_pick)\n",
    "print(data.decode([target]), ' : Actual Target (', target, ')')\n",
    "print(guess, \" : Educated guess by picking amongst the highest probabilities (\", idx_guess[0], ')')\n",
    "print(pick, \" : Pick based solely on highest probability (\", idx_pick[0], ')')\n",
    "\n",
    "# Re running this block will re run the input through the model (note that this is just inference and not training so it does not learn the answer)\n",
    "# after multiple runs the probability pick should not change however the educated guess might guess the proper value\n",
    "# in addition to increase the likely hood of success run the training loop for more iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits: \n",
      " tensor([[ 4.3867e+00,  8.3823e+00, -2.1884e+00, -4.7520e+00, -4.6742e+00,\n",
      "          1.9149e+00,  7.4428e-01,  3.1786e+00, -7.1796e-01, -4.3645e+00,\n",
      "         -1.2102e-01, -1.1285e+00, -1.7564e+00, -5.6405e+00, -2.9424e+00,\n",
      "         -1.5828e+00, -4.9551e+00, -4.6845e+00, -3.1202e+00, -2.6425e+00,\n",
      "         -3.0984e+00, -2.3559e+00, -5.1862e+00, -3.6260e+00, -2.4561e+00,\n",
      "         -3.5230e+00, -2.9129e+00, -3.1595e+00, -4.3750e+00, -5.9201e+00,\n",
      "         -2.5016e+00, -2.3055e+00, -2.4559e+00, -5.1129e+00, -1.7377e+00,\n",
      "         -2.8831e+00, -3.7401e+00, -4.8751e+00, -3.5875e+00, -8.8616e-01,\n",
      "         -6.4739e-01,  6.1424e-01,  1.1200e+00, -4.3336e-01,  3.4910e-01,\n",
      "          2.2319e-03, -1.7371e+00,  1.2398e+00, -1.7297e+00, -4.6964e-01,\n",
      "          7.8091e-02,  1.3279e+00,  1.9822e+00,  2.5495e-01, -2.5736e-01,\n",
      "         -1.8223e+00,  1.7114e+00,  9.6151e-01,  4.1667e-01,  1.8212e-01,\n",
      "          1.0968e+00,  8.2465e-01, -2.1205e+00, -8.7777e-01, -4.0378e+00]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Probs: \n",
      " tensor([[1.7743e-02, 9.6445e-01, 2.4743e-05, 1.9060e-06, 2.0601e-06, 1.4979e-03,\n",
      "         4.6464e-04, 5.3004e-03, 1.0767e-04, 2.8079e-06, 1.9558e-04, 7.1413e-05,\n",
      "         3.8116e-05, 7.8388e-07, 1.1642e-05, 4.5342e-05, 1.5557e-06, 2.0390e-06,\n",
      "         9.7449e-06, 1.5714e-05, 9.9598e-06, 2.0929e-05, 1.2347e-06, 5.8769e-06,\n",
      "         1.8932e-05, 6.5144e-06, 1.1991e-05, 9.3695e-06, 2.7786e-06, 5.9265e-07,\n",
      "         1.8090e-05, 2.2010e-05, 1.8936e-05, 1.3285e-06, 3.8834e-05, 1.2353e-05,\n",
      "         5.2430e-06, 1.6852e-06, 6.1075e-06, 9.0997e-05, 1.1554e-04, 4.0798e-04,\n",
      "         6.7654e-04, 1.4311e-04, 3.1296e-04, 2.2123e-04, 3.8858e-05, 7.6264e-04,\n",
      "         3.9145e-05, 1.3801e-04, 2.3867e-04, 8.3284e-04, 1.6023e-03, 2.8484e-04,\n",
      "         1.7065e-04, 3.5685e-05, 1.2222e-03, 5.7738e-04, 3.3484e-04, 2.6484e-04,\n",
      "         6.6104e-04, 5.0352e-04, 2.6483e-05, 9.1764e-05, 3.8931e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Probs: \n",
      " [['1.77%', '96.45%', '0.00%', '0.00%', '0.00%', '0.15%', '0.05%', '0.53%', '0.01%', '0.00%', '0.02%', '0.01%', '0.00%', '0.00%', '0.00%', '0.00%', '0.00%', '0.00%', '0.00%', '0.00%', '0.00%', '0.00%', '0.00%', '0.00%', '0.00%', '0.00%', '0.00%', '0.00%', '0.00%', '0.00%', '0.00%', '0.00%', '0.00%', '0.00%', '0.00%', '0.00%', '0.00%', '0.00%', '0.00%', '0.01%', '0.01%', '0.04%', '0.07%', '0.01%', '0.03%', '0.02%', '0.00%', '0.08%', '0.00%', '0.01%', '0.02%', '0.08%', '0.16%', '0.03%', '0.02%', '0.00%', '0.12%', '0.06%', '0.03%', '0.03%', '0.07%', '0.05%', '0.00%', '0.01%', '0.00%']]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "def format_probs_tensor_as_percent(probs_tensor):\n",
    "    \"\"\"\n",
    "    Converts a PyTorch tensor of probabilities into a percentage format for display.\n",
    "    Handles 2D tensors by processing each row separately.\n",
    "\n",
    "    :param probs_tensor: PyTorch tensor containing probability values (0 to 1).\n",
    "    :return: List of formatted percentages as strings.\n",
    "    \"\"\"\n",
    "    probs_percent = (probs_tensor * 100).to(torch.float32)  # Convert to percentage scale\n",
    "    \n",
    "    # Convert each probability to a formatted string with two decimal places\n",
    "    formatted_probs = [[f\"{float(p):.2f}%\" for p in row] for row in probs_percent]\n",
    "\n",
    "    return formatted_probs\n",
    "\n",
    "print(\"logits: \\n\", logits)\n",
    "print('Probs: \\n', probs)\n",
    "\n",
    "print('Probs formatted: \\n', format_probs_tensor_as_percent(probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of model logits:  torch.Size([1, 64, 65])\n",
      "model logits: \n",
      " tensor([[[ 5.7792,  7.8561, -1.3397,  ..., -5.3172, -0.5288, -6.9107],\n",
      "         [-2.4496, -2.7531, -4.8655,  ..., -3.0714,  2.6392, -3.7142],\n",
      "         [-1.2435,  0.0179, -2.2988,  ..., -3.4009,  2.1964, -3.9612],\n",
      "         ...,\n",
      "         [-1.2935,  0.8852, -1.9491,  ...,  0.0915, -0.5228,  1.4113],\n",
      "         [ 0.3237,  3.1504,  0.5962,  ..., -1.8156,  2.2368, -1.6596],\n",
      "         [ 4.3867,  8.3823, -2.1884,  ..., -2.1205, -0.8778, -4.0378]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# see the raw model logits, this shows the value of the logits at all positions and not just the final position that we are uttilizing for projection of the final answer\n",
    "\n",
    "print(\"Size of model logits: \", mod_logits.size()) # Batch, Block_Size, Vocabulary \n",
    "print(\"model logits: \\n\", mod_logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Educated Guess:  3\n",
      "Highest Probability:  2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "#This block shows a sample of the difference between using an \"Educated Guess\" vs a \"Highest Probability\"\n",
    "\n",
    "# Define probability distribution\n",
    "probs = torch.tensor([0.1, 0.3, 0.4, 0.2])  # Must be non-negative\n",
    "\n",
    "# Educated Guess: Sample 1 events without replacement \n",
    "educated_guess = torch.multinomial(probs, num_samples=1, replacement=False).item()  # using .item() becasue we are only doing a single sample for comparis\n",
    "\n",
    "highest_probablity = torch.argmax(probs).item() \n",
    "\n",
    "print('Educated Guess: ', educated_guess) \n",
    "print('Highest Probability: ', highest_probablity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
