{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found vocab_size = 65 (inside data\\shakespeare\\meta.pkl)\n",
      "number of parameters: 0.21M\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#this sample is adapted from the work provided here: https://github.com/karpathy/nanoGPT\n",
    "#removed some of the more complicated concepts (learning rate decay, gradiant accumulation steps, dps support, configuration management etc for a clearer sample)\n",
    "#refactored to create a training class responsible for training the model, and a dataloader to fetch data\n",
    "\n",
    "# There are 2 datasets to train from, the character representation of shakespeare which yields a vocab size of 65\n",
    "# The toy dataset which has a vocabulary of 11\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from config import ToyConfig, ToyTrainingConfig\n",
    "from config import ToyTrainingConfig\n",
    "from toy_model import ToyModel\n",
    "from data_loader import DataLoader\n",
    "from toy_trainer import ToyTrainer\n",
    "\n",
    "modelConfig = ToyConfig()\n",
    "#override any default values here: \n",
    "modelConfig.block_size= 64\n",
    "modelConfig.n_embd = 128 # Number of C elements in the B, T, C Tensor that is used in the model ( B = Batch Size, T = Time position or Sequence Length, C = Number of embed elements )\n",
    "\n",
    "trainingConfig = ToyTrainingConfig()\n",
    "trainingConfig.block_size = modelConfig.block_size # Need to set to ToyConfig.block_size\n",
    "trainingConfig.learning_rate= 1e-3\n",
    "\n",
    "# name of the dataset to use ('toy' or 'shakespeare')\n",
    "dataset = 'shakespeare'\n",
    "\n",
    "data = DataLoader(dataset=dataset)\n",
    "modelConfig.vocab_size = data.get_vocab_size()\n",
    "trainingConfig.vocab_size = modelConfig.vocab_size\n",
    "model = ToyModel(modelConfig)\n",
    "trainer = ToyTrainer(trainingConfig, model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: loss 4.1845, time 174.81ms\n",
      "iter 100: loss 2.5368, time 69.34ms\n",
      "iter 200: loss 2.3905, time 83.01ms\n",
      "iter 300: loss 2.2459, time 68.36ms\n",
      "iter 400: loss 2.1227, time 88.88ms\n",
      "iter 500: loss 2.0885, time 86.91ms\n",
      "iter 600: loss 2.0424, time 81.06ms\n",
      "iter 700: loss 1.9779, time 98.64ms\n",
      "iter 800: loss 1.8516, time 91.81ms\n",
      "iter 900: loss 1.8270, time 92.77ms\n",
      "iter 1000: loss 1.8359, time 84.96ms\n",
      "iter 1100: loss 1.7775, time 89.85ms\n",
      "iter 1200: loss 1.7581, time 87.89ms\n"
     ]
    }
   ],
   "source": [
    "#train the model some so that the results aren't random a loss of 1.55 is acheivable with enough training with block_size 64, n_embed of 128\n",
    "#the learning rate is too high which stops this model from acheiving it's max training loss of 1.48 (validation (1.67) \n",
    "\n",
    "#Training for more iterations will increase the probability of a correct answer try with larger trainning iterations to see the improvement (to the limits of the model / learning rate):\n",
    "\n",
    "#shakespeare\n",
    "#batch_size, number of iterations, log every x iterations\n",
    "trainer.TrainingLoop(24, 1200, 100) #1200 iterations takes about 1.5 min and yields about 1.7 loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t  : Target single Character according to source input \n",
      "\n",
      "Input: \n",
      "  thou more murmur'st, I will rend an oak\n",
      "And peg thee in his kno\n",
      "\n",
      "Target: \n",
      " thou more murmur'st, I will rend an oak\n",
      "And peg thee in his knot\n"
     ]
    }
   ],
   "source": [
    "# you can repeat this as nessary until you get a good input\n",
    "X, Y = trainer.get_single_batch() #get_single_batch gets validation data so this is data the model has not seen during training\n",
    "\n",
    "B, T = Y.size()\n",
    "target = Y[0][T-1].tolist()\n",
    "print(data.decode([target]), ' : Target single Character according to source input \\n')\n",
    "\n",
    "# The target is one character less then the target, the number of characters shown is based on block size\n",
    "print(\"Input: \\n\",data.decode(X.flatten().tolist()))\n",
    "print(\"\\nTarget: \\n\",data.decode(Y.flatten().tolist()))\n",
    "\n",
    "#print(\"\\nX: \\n\", X)\n",
    "#print(\"\\nY: \\n\", Y)\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of the Model vs the Target input: **Note: rerunning this will do a backwards pass and thus improve the output \n",
      "\n",
      "  -> [\"'\\n':2.5%\", \"' ':90.3%\", \"'!':0.2%\", \"'$':0.0%\", \"'&':0.0%\", \"''':0.2%\", \"',':1.1%\", \"'-':0.0%\", \"'.':0.8%\", \"'3':0.0%\", \"':':0.6%\", \"';':0.2%\", \"'?':0.5%\", \"'A':0.0%\", \"'B':0.0%\", \"'C':0.0%\", \"'D':0.0%\", \"'E':0.0%\", \"'F':0.0%\", \"'G':0.0%\", \"'H':0.0%\", \"'I':0.0%\", \"'J':0.0%\", \"'K':0.0%\", \"'L':0.0%\", \"'M':0.0%\", \"'N':0.0%\", \"'O':0.0%\", \"'P':0.0%\", \"'Q':0.0%\", \"'R':0.0%\", \"'S':0.0%\", \"'T':0.1%\", \"'U':0.0%\", \"'V':0.0%\", \"'W':0.0%\", \"'X':0.0%\", \"'Y':0.0%\", \"'Z':0.0%\", \"'a':0.0%\", \"'b':0.0%\", \"'c':0.0%\", \"'d':0.1%\", \"'e':0.6%\", \"'f':0.0%\", \"'g':0.0%\", \"'h':1.9%\", \"'i':0.0%\", \"'j':0.0%\", \"'k':0.0%\", \"'l':0.0%\", \"'m':0.0%\", \"'n':0.0%\", \"'o':0.0%\", \"'p':0.0%\", \"'q':0.0%\", \"'r':0.0%\", \"'s':0.1%\", \"'t':0.6%\", \"'u':0.0%\", \"'v':0.0%\", \"'w':0.0%\", \"'x':0.0%\", \"'y':0.0%\", \"'z':0.0%\"]\n",
      "t -> [\"'\\n':0.0%\", \"' ':0.0%\", \"'!':0.0%\", \"'$':0.0%\", \"'&':0.0%\", \"''':0.2%\", \"',':0.0%\", \"'-':0.0%\", \"'.':0.0%\", \"'3':0.0%\", \"':':0.0%\", \"';':0.0%\", \"'?':0.0%\", \"'A':0.1%\", \"'B':0.1%\", \"'C':0.1%\", \"'D':0.1%\", \"'E':0.0%\", \"'F':0.0%\", \"'G':0.1%\", \"'H':0.0%\", \"'I':0.4%\", \"'J':0.0%\", \"'K':0.0%\", \"'L':0.1%\", \"'M':0.1%\", \"'N':0.0%\", \"'O':0.1%\", \"'P':0.1%\", \"'Q':0.0%\", \"'R':0.1%\", \"'S':0.1%\", \"'T':0.1%\", \"'U':0.0%\", \"'V':0.0%\", \"'W':0.1%\", \"'X':0.0%\", \"'Y':0.0%\", \"'Z':0.0%\", \"'a':5.9%\", \"'b':4.4%\", \"'c':1.6%\", \"'d':8.6%\", \"'e':0.8%\", \"'f':1.7%\", \"'g':0.9%\", \"'h':3.5%\", \"'i':2.0%\", \"'j':0.1%\", \"'k':0.6%\", \"'l':1.2%\", \"'m':7.6%\", \"'n':3.4%\", \"'o':2.1%\", \"'p':9.0%\", \"'q':0.2%\", \"'r':1.2%\", \"'s':8.8%\", \"'t':30.2%\", \"'u':0.4%\", \"'v':0.8%\", \"'w':1.3%\", \"'x':0.0%\", \"'y':1.5%\", \"'z':0.0%\"]\n",
      "n -> [\"'\\n':0.2%\", \"' ':0.4%\", \"'!':0.1%\", \"'$':0.0%\", \"'&':0.0%\", \"''':0.4%\", \"',':0.2%\", \"'-':0.0%\", \"'.':0.1%\", \"'3':0.0%\", \"':':0.1%\", \"';':0.1%\", \"'?':0.1%\", \"'A':0.0%\", \"'B':0.0%\", \"'C':0.0%\", \"'D':0.0%\", \"'E':0.0%\", \"'F':0.0%\", \"'G':0.0%\", \"'H':0.0%\", \"'I':0.0%\", \"'J':0.0%\", \"'K':0.0%\", \"'L':0.0%\", \"'M':0.0%\", \"'N':0.0%\", \"'O':0.0%\", \"'P':0.0%\", \"'Q':0.0%\", \"'R':0.0%\", \"'S':0.0%\", \"'T':0.0%\", \"'U':0.0%\", \"'V':0.0%\", \"'W':0.0%\", \"'X':0.0%\", \"'Y':0.0%\", \"'Z':0.0%\", \"'a':0.2%\", \"'b':0.0%\", \"'c':0.1%\", \"'d':0.0%\", \"'e':0.2%\", \"'f':0.1%\", \"'g':0.0%\", \"'h':0.0%\", \"'i':6.7%\", \"'j':0.0%\", \"'k':0.0%\", \"'l':0.0%\", \"'m':0.0%\", \"'n':89.7%\", \"'o':0.1%\", \"'p':0.3%\", \"'q':0.0%\", \"'r':0.0%\", \"'s':0.2%\", \"'t':0.1%\", \"'u':0.1%\", \"'v':0.0%\", \"'w':0.0%\", \"'x':0.1%\", \"'y':0.0%\", \"'z':0.0%\"]\n",
      "o -> [\"'\\n':0.7%\", \"' ':2.8%\", \"'!':0.3%\", \"'$':0.0%\", \"'&':0.0%\", \"''':0.6%\", \"',':1.1%\", \"'-':0.3%\", \"'.':0.7%\", \"'3':0.0%\", \"':':0.1%\", \"';':0.7%\", \"'?':0.4%\", \"'A':0.0%\", \"'B':0.0%\", \"'C':0.0%\", \"'D':0.0%\", \"'E':0.0%\", \"'F':0.0%\", \"'G':0.0%\", \"'H':0.0%\", \"'I':0.0%\", \"'J':0.0%\", \"'K':0.0%\", \"'L':0.0%\", \"'M':0.0%\", \"'N':0.0%\", \"'O':0.1%\", \"'P':0.0%\", \"'Q':0.0%\", \"'R':0.0%\", \"'S':0.0%\", \"'T':0.0%\", \"'U':0.0%\", \"'V':0.0%\", \"'W':0.0%\", \"'X':0.0%\", \"'Y':0.0%\", \"'Z':0.0%\", \"'a':8.6%\", \"'b':0.1%\", \"'c':0.4%\", \"'d':0.2%\", \"'e':4.8%\", \"'f':0.5%\", \"'g':0.2%\", \"'h':0.4%\", \"'i':5.5%\", \"'j':0.1%\", \"'k':0.2%\", \"'l':0.4%\", \"'m':0.0%\", \"'n':0.7%\", \"'o':64.0%\", \"'p':0.5%\", \"'q':0.2%\", \"'r':0.0%\", \"'s':0.1%\", \"'t':0.4%\", \"'u':4.4%\", \"'v':0.1%\", \"'w':0.1%\", \"'x':0.1%\", \"'y':0.0%\", \"'z':0.1%\"]\n",
      "t -> [\"'\\n':0.2%\", \"' ':6.0%\", \"'!':0.0%\", \"'$':0.0%\", \"'&':0.0%\", \"''':0.4%\", \"',':0.3%\", \"'-':0.1%\", \"'.':0.0%\", \"'3':0.0%\", \"':':0.1%\", \"';':0.1%\", \"'?':0.0%\", \"'A':0.0%\", \"'B':0.0%\", \"'C':0.0%\", \"'D':0.0%\", \"'E':0.0%\", \"'F':0.0%\", \"'G':0.0%\", \"'H':0.0%\", \"'I':0.0%\", \"'J':0.0%\", \"'K':0.0%\", \"'L':0.0%\", \"'M':0.0%\", \"'N':0.0%\", \"'O':0.0%\", \"'P':0.0%\", \"'Q':0.0%\", \"'R':0.0%\", \"'S':0.0%\", \"'T':0.1%\", \"'U':0.0%\", \"'V':0.0%\", \"'W':0.0%\", \"'X':0.0%\", \"'Y':0.0%\", \"'Z':0.0%\", \"'a':0.1%\", \"'b':1.8%\", \"'c':0.4%\", \"'d':0.0%\", \"'e':0.1%\", \"'f':0.1%\", \"'g':0.1%\", \"'h':0.0%\", \"'i':0.1%\", \"'j':0.0%\", \"'k':0.0%\", \"'l':0.1%\", \"'m':1.0%\", \"'n':3.6%\", \"'o':0.1%\", \"'p':0.3%\", \"'q':0.0%\", \"'r':0.7%\", \"'s':0.6%\", \"'t':71.8%\", \"'u':1.3%\", \"'v':0.4%\", \"'w':9.7%\", \"'x':0.0%\", \"'y':0.1%\", \"'z':0.0%\"]\n",
      "\n",
      "Data Target (decoded): \n",
      " thou more murmur'st, I will rend an oak\n",
      "And peg thee in his knot\n",
      "\n",
      "Model Result (decoded): \n",
      " n au ture auseases   a dill sipd t  tf  And ticeaha  an tis tnot\n",
      "\n",
      "Last Char of the Target: t\n",
      "Last Char of the  Model: t\n",
      "\n",
      "This pass: Succeeded\n",
      "\n",
      "Loss:  1.7466986179351807\n"
     ]
    }
   ],
   "source": [
    "# everytime this step is run the model will learn the input\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def format_probs_tensor_as_percent(probs_tensor):\n",
    "    \"\"\"\n",
    "    Converts a PyTorch tensor of probabilities into a percentage format for display.\n",
    "    If probs_tensor is 1D (shape [vocab_size]), we treat it as a single row.\n",
    "    If it's 2D (shape [N, vocab_size]), we handle multiple rows.\n",
    "    \n",
    "    :param probs_tensor: PyTorch tensor containing probability values (0 to 1).\n",
    "    :return: A list of lists of formatted percentage strings.\n",
    "    \"\"\"\n",
    "    # 1) Convert to percentage scale and cast to float\n",
    "    probs_percent = (probs_tensor * 100).to(torch.float32)\n",
    "    \n",
    "    # 2) Ensure 2D for uniform processing\n",
    "    #    If shape is (vocab_size,), make it (1, vocab_size)\n",
    "    if probs_percent.dim() == 1:\n",
    "        probs_percent = probs_percent.unsqueeze(0)\n",
    "    \n",
    "    # Build the vocabulary array (assuming modelConfig.vocab_size and data are in scope)\n",
    "    vocab_arr = [data.decode([i]) for i in range(modelConfig.vocab_size)]\n",
    "\n",
    "    # 3) Convert each probability to a formatted string with two decimal places\n",
    "    #    We'll return a 2D list, one sub-list per row\n",
    "    formatted_probs = [\n",
    "        [\n",
    "            f\"'{vocab_arr[idx]}':{float(prob):.1f}%\"\n",
    "            for idx, prob in enumerate(row)\n",
    "        ]\n",
    "        for row in probs_percent\n",
    "    ]\n",
    "    \n",
    "    return formatted_probs\n",
    "\n",
    "def format_model_results(target, logits, printCount):\n",
    "    # Convert logits -> probabilities\n",
    "    allProbs = F.softmax(logits, dim=-1)  # shape: (B, T, vocab_size)\n",
    "    allPicks = torch.argmax(allProbs, dim=-1)  # shape: (B, T)\n",
    "\n",
    "    # batch size = 1 for simplicity\n",
    "    B, T, V = allProbs.shape\n",
    "    assert B == 1, \"This snippet assumes batch size = 1\"\n",
    "\n",
    "    # Determine the range for the last 'printCount' positions\n",
    "    #    e.g. if T=10 and printCount=4, we want positions [6..9].\n",
    "    start_idx = max(0, T - printCount)\n",
    "    \n",
    "    for i in range(start_idx, T):\n",
    "        # The chosen token index at position i\n",
    "        pick_idx = allPicks[0, i].item()\n",
    "        # Decode that index into a character\n",
    "        char = data.decode([pick_idx])\n",
    "        \n",
    "        # 3) Format probabilities for that position\n",
    "        #    shape (vocab_size,) => pass to our function\n",
    "        probs_i = allProbs[0, i]\n",
    "        \n",
    "        # format_probs_tensor_as_percent returns a list of lists;\n",
    "        # with a single row, we can index [0] to get the list of per-token strings\n",
    "        formatted = format_probs_tensor_as_percent(probs_i)[0]\n",
    "        \n",
    "        print(f\"{char} -> {formatted}\")\n",
    "\n",
    "    # 3) Decode the entire sequences\n",
    "    decoded_target = data.decode(target.flatten().tolist())\n",
    "    decoded_model  = data.decode(allPicks.flatten().tolist())\n",
    "\n",
    "    print(\"\\nData Target (decoded): \\n\", decoded_target)\n",
    "    print(\"\\nModel Result (decoded): \\n\", decoded_model)\n",
    "\n",
    "    # 4) Print the last character of each\n",
    "    print(f\"\\nLast Char of the Target: {decoded_target[-1]}\")    \n",
    "    print(f\"Last Char of the  Model: {decoded_model[-1]}\")\n",
    "\n",
    "    success = decoded_target[-1] == decoded_model[-1]\n",
    "    resutlStr = 'Succeeded' if success else 'Failed'\n",
    "    print('\\nThis pass: '+ resutlStr)\n",
    "\n",
    "# -- After running the model --\n",
    "logits, loss = model(X, Y) \n",
    "\n",
    "print('Results of the Model vs the Target input: **Note: rerunning this will do a backwards pass and thus improve the output \\n')\n",
    "\n",
    "format_model_results(Y, logits, 5)\n",
    "\n",
    "print('\\nLoss: ', loss.item())\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "trainer.optimizer.step()\n",
    "\n",
    "# flush the gradients as soon as we can, no need for this memory anymore\n",
    "trainer.optimizer.zero_grad(set_to_none=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of model logits:  torch.Size([1, 64, 65])\n",
      "model logits: \n",
      " tensor([[[ 1.7899,  0.6305, -3.7608,  ..., -2.5100,  0.1603, -4.7069],\n",
      "         [ 1.5309,  4.0947, -0.2506,  ..., -2.7289, -0.6738, -4.0253],\n",
      "         [-0.1413,  1.7320, -1.7145,  ..., -1.2339,  4.0117, -3.9531],\n",
      "         ...,\n",
      "         [ 0.7147,  1.1776, -0.1299,  ...,  0.9869, -1.5470, -0.7838],\n",
      "         [-0.2087,  1.3126, -0.0595,  ..., -1.3608, -0.1276, -0.8348],\n",
      "         [-0.9568,  3.7113, -0.6027,  ..., -1.3182,  0.2163, -1.9614]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# see the raw model logits, this shows the value of the logits at all positions and not just the final position that we are uttilizing for projection of the final answer\n",
    "\n",
    "print(\"Size of model logits: \", logits.size()) # Batch, Block_Size, Vocabulary \n",
    "print(\"model logits: \\n\", logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Educated Guess:  2\n",
      "Highest Probability:  2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "#This block shows a sample of the difference between using an \"Educated Guess\" vs a \"Highest Probability\"\n",
    "\n",
    "# Define probability distribution\n",
    "probs = torch.tensor([0.1, 0.3, 0.4, 0.2])  # Must be non-negative\n",
    "\n",
    "# Educated Guess: Sample 1 events without replacement \n",
    "educated_guess = torch.multinomial(probs, num_samples=1, replacement=False).item()  # using .item() becasue we are only doing a single sample for comparis\n",
    "\n",
    "highest_probablity = torch.argmax(probs).item() \n",
    "\n",
    "print('Educated Guess: ', educated_guess) \n",
    "print('Highest Probability: ', highest_probablity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
